# mlp

lightweight ML inference server with sockets


## Getting Started

Best way to run server is to run the `docker-compose.yml` file:

```sh
docker-compose -f build/Dockerfile up --build
```
